{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YelpSentimentAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emilypeek1/YelpSentimentAnalysis/blob/master/YelpSentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgbThMXPe1UX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "46fe595a-e3f5-4628-d0da-a4a22c590585"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLRnJCKhWLEa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "INPUT_FOLDER = \"/content/drive/My Drive/\"\n",
        "\n",
        "def load_yelp_orig_data():\n",
        "  # read the entire file into a python array\n",
        "  with open(INPUT_FOLDER + 'yelp_academic_dataset_review.json', 'r') as f:\n",
        "      data = f.readlines()\n",
        "\n",
        "  # remove the trailing \"\\n\" from each line\n",
        "  data = map(lambda x: x.rstrip(), data)\n",
        "\n",
        "  data_json_str = \"[\" + ','.join(data) + \"]\"\n",
        "\n",
        "  size = 1000000\n",
        "  review = pd.read_json(data_json_str, lines=True,\n",
        "                        dtype={'review_id':str,'user_id':str,\n",
        "                              'business_id':str,'stars':int,\n",
        "                              'useful':int,'funny':int,'cool':int,\n",
        "                              'text':str,'date':str},\n",
        "                        chunksize=size)\n",
        "  \n",
        "  # There are multiple chunks to be read\n",
        "  for chunk_review in review:\n",
        "    # Drop columns that aren't needed\n",
        "    chunk_review = chunk_review.drop(['review_id', 'user_id', 'business_id', 'useful', 'funny', 'cool', 'date'], axis=1)\n",
        "    # Drop rows with missing values\n",
        "    chunk_review = chunk_review.dropna()\n",
        "    # Add chunk\n",
        "    chunk_list.append(chunk_review)\n",
        "  # After trimming down the review file, concatenate all relevant data back to one dataframe\n",
        "  df = pd.concat(chunk_list, ignore_index=True, join='outer', axis=0)\n",
        "  # convert to csv\n",
        "  csv_name = \"yelp_reviews.csv\"\n",
        "  df.to_csv(INPUT_FOLDER + csv_name, index=False)\n",
        "\n",
        "load_yelp_orig_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyVL_YEd6Pft",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "ed123958-15d2-4153-eeaa-e8d8c2b8d04c"
      },
      "source": [
        "top_data_df = pd.read_csv(INPUT_FOLDER + 'yelp_reviews.csv', nrows=2000000, usecols=[\"stars\"])\n",
        "print(\"Columns in the original dataset:\\n\")\n",
        "print(top_data_df.dtypes)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Columns in the original dataset:\n",
            "\n",
            "stars    object\n",
            "dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQ2W_VxguZw3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        },
        "outputId": "d25b6f57-6e4a-4224-a610-dff0f6babc3b"
      },
      "source": [
        "weird = (top_data_df.applymap(type) != top_data_df.iloc[0].apply(type)).any(axis=1)\n",
        "print(top_data_df[weird])\n",
        "top_data_df.stars = top_data_df.stars.astype('int')\n",
        "#print(top_data_df.iloc[1048575, 0].dtype)\n",
        "print(top_data_df.iloc[1048576, 0] > 0)\n",
        "\n",
        "#top_data_df.stars = top_data_df.stars.astype(np.int64)\n",
        "#print(top_data_df.iloc[1048576, 0].dtype)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        stars\n",
            "1048576     2\n",
            "1048577     5\n",
            "1048578     2\n",
            "1048579     5\n",
            "1048580     1\n",
            "...       ...\n",
            "1835003     4\n",
            "1835004     3\n",
            "1835005     5\n",
            "1835006     5\n",
            "1835007     1\n",
            "\n",
            "[524288 rows x 1 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-5dd139a945e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mweird\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtop_data_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtop_data_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_data_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mweird\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtop_data_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_data_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#print(top_data_df.iloc[1048575, 0].dtype)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_data_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1048576\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   5696\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5697\u001b[0m             \u001b[0;31m# else, only a single dtype is given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5698\u001b[0;31m             \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5699\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"raise\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"astype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, filter, **kwargs)\u001b[0m\n\u001b[1;32m    440\u001b[0m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0mvals1d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_nansafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals1d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0;31m# e.g. astype_nansafe can fail on object-dtype of strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mastype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m    872\u001b[0m         \u001b[0;31m# work around NumPy brokenness, #1987\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype_intsafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m         \u001b[0;31m# if we have a datetime/timedelta array of objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.astype_intsafe\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'All the goodness of the deep South! Seasoned and pan bronzed. Mississippi catfish served over Gouda grits with an andouille sausage and mushroom ragout.'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fb9MBxXZvvEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdYSkSxGWO93",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt \n",
        "\n",
        "print(\"Number of rows per star rating:\")\n",
        "print(top_data_df['stars'].value_counts())\n",
        "\n",
        "# Function to map stars to sentiment\n",
        "def map_sentiment(stars_received):\n",
        "    if stars_received <= 2:\n",
        "        return -1\n",
        "    elif stars_received == 3:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "# Mapping stars to sentiment into three categories\n",
        "top_data_df['sentiment'] = [ map_sentiment(x) for x in top_data_df['stars']]\n",
        "# Plotting the sentiment distribution\n",
        "plt.figure()\n",
        "pd.value_counts(top_data_df['sentiment']).plot.bar(title=\"Sentiment distribution in df\")\n",
        "plt.xlabel(\"Sentiment\")\n",
        "plt.ylabel(\"No. of rows in df\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72l-0eqcWVsY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to retrieve top few number of each category\n",
        "def get_top_data(top_n = 5000):\n",
        "    top_data_df_positive = top_data_df[top_data_df['sentiment'] == 1].head(top_n)\n",
        "    top_data_df_negative = top_data_df[top_data_df['sentiment'] == -1].head(top_n)\n",
        "    top_data_df_neutral = top_data_df[top_data_df['sentiment'] == 0].head(top_n)\n",
        "    top_data_df_small = pd.concat([top_data_df_positive, top_data_df_negative, top_data_df_neutral])\n",
        "    return top_data_df_small\n",
        "\n",
        "# Function call to get the top 10000 from each sentiment\n",
        "top_data_df_small = get_top_data(top_n=10000)\n",
        "\n",
        "# After selecting top few samples of each sentiment\n",
        "print(\"After segregating and taking equal number of rows for each sentiment:\")\n",
        "print(top_data_df_small['sentiment'].value_counts())\n",
        "top_data_df_small.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckheSTYTWWeA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.utils import simple_preprocess\n",
        "# Tokenize the text column to get the new column 'tokenized_text'\n",
        "top_data_df_small['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in top_data_df_small['text']] \n",
        "print(top_data_df_small['tokenized_text'].head(10))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhYz2bfciT5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lemmatize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBEhmXvUWe3k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.parsing.porter import PorterStemmer\n",
        "porter_stemmer = PorterStemmer()\n",
        "# Get the stemmed_tokens\n",
        "top_data_df_small['stemmed_tokens'] = [[porter_stemmer.stem(word) for word in tokens] for tokens in top_data_df_small['tokenized_text'] ]\n",
        "top_data_df_small['stemmed_tokens'].head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fi2uCepXbbNB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compute tf-idf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3GUTi87Wilm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Train Test Split Function\n",
        "def split_train_test(top_data_df_small, test_size=0.3, shuffle_state=True):\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(top_data_df_small[['stars', 'stemmed_tokens']], \n",
        "                                                        top_data_df_small['sentiment'], \n",
        "                                                        shuffle=shuffle_state,\n",
        "                                                        test_size=test_size, \n",
        "                                                        random_state=15)\n",
        "    print(\"Value counts for Train sentiments\")\n",
        "    print(Y_train.value_counts())\n",
        "    print(\"Value counts for Test sentiments\")\n",
        "    print(Y_test.value_counts())\n",
        "    print(type(X_train))\n",
        "    print(type(Y_train))\n",
        "    X_train = X_train.reset_index()\n",
        "    X_test = X_test.reset_index()\n",
        "    Y_train = Y_train.to_frame()\n",
        "    Y_train = Y_train.reset_index()\n",
        "    Y_test = Y_test.to_frame()\n",
        "    Y_test = Y_test.reset_index()\n",
        "    print(X_train.head())\n",
        "    return X_train, X_test, Y_train, Y_test\n",
        "\n",
        "# Call the train_test_split\n",
        "X_train, X_test, Y_train, Y_test = split_train_test(top_data_df_small)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-Jx9jd-Wmna",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "import time\n",
        "# Skip-gram model (sg = 1)\n",
        "size = 1000\n",
        "window = 3\n",
        "min_count = 1\n",
        "workers = 3\n",
        "sg = 1\n",
        "\n",
        "word2vec_model_file = INPUT_FOLDER + 'word2vec_' + str(size) + '.model'\n",
        "start_time = time.time()\n",
        "stemmed_tokens = pd.Series(top_data_df_small['stemmed_tokens']).values\n",
        "# Train the Word2Vec Model\n",
        "w2v_model = Word2Vec(stemmed_tokens, min_count = min_count, size = size, workers = workers, window = window, sg = sg)\n",
        "print(\"Time taken to train word2vec model: \" + str(time.time() - start_time))\n",
        "w2v_model.save(word2vec_model_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmB2dmCvWnXK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "# Load the model from the model file\n",
        "sg_w2v_model = Word2Vec.load(word2vec_model_file)\n",
        "# Unique ID of the word\n",
        "print(\"Index of the word 'action':\")\n",
        "print(sg_w2v_model.wv.vocab[\"action\"].index)\n",
        "# Total number of the words \n",
        "print(len(sg_w2v_model.wv.vocab))\n",
        "# Print the size of the word2vec vector for one word\n",
        "print(\"Length of the vector generated for a word\")\n",
        "print(len(sg_w2v_model['action']))\n",
        "# Get the mean for the vectors for an example review\n",
        "print(\"Print the length after taking average of all word vectors in a sentence:\")\n",
        "print(np.mean([sg_w2v_model[token] for token in top_data_df_small['stemmed_tokens'][0]], axis=0))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyeE_CyWWqYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Store the vectors for train data in following file\n",
        "word2vec_filename = INPUT_FOLDER + 'train_review_word2vec.csv'\n",
        "with open(word2vec_filename, 'w+') as word2vec_file:\n",
        "    for index, row in X_train.iterrows():\n",
        "        model_vector = (np.mean([sg_w2v_model[token] for token in row['stemmed_tokens']], axis=0)).tolist()\n",
        "        if index == 0:\n",
        "            header = \",\".join(str(ele) for ele in range(1000))\n",
        "            word2vec_file.write(header)\n",
        "            word2vec_file.write(\"\\n\")\n",
        "        # Check if the line exists else it is vector of zeros\n",
        "        if type(model_vector) is list:  \n",
        "            line1 = \",\".join( [str(vector_element) for vector_element in model_vector] )\n",
        "        else:\n",
        "            line1 = \",\".join([str(0) for i in range(1000)])\n",
        "        word2vec_file.write(line1)\n",
        "        word2vec_file.write('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZkOtNxHWvS6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "#Import the DecisionTreeeClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# Load from the filename\n",
        "word2vec_df = pd.read_csv(word2vec_filename)\n",
        "#Initialize the model\n",
        "clf_decision_word2vec = DecisionTreeClassifier()\n",
        "\n",
        "start_time = time.time()\n",
        "# Fit the model\n",
        "clf_decision_word2vec.fit(word2vec_df, Y_train['sentiment'])\n",
        "print(\"Time taken to fit the model with word2vec vectors: \" + str(time.time() - start_time))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0CmHzuSiqC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# naive bayes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knoyRilNisLQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# random forest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kGZxb8liu1o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# linear SVM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSV3oxkiiwmd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# neural network"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkGFNaR4Wyc1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "test_features_word2vec = []\n",
        "for index, row in X_test.iterrows():\n",
        "    model_vector = np.mean([sg_w2v_model[token] for token in row['stemmed_tokens']], axis=0)\n",
        "    if type(model_vector) is list:\n",
        "        test_features_word2vec.append(model_vector)\n",
        "    else:\n",
        "        test_features_word2vec.append(np.array([0 for i in range(1000)]))\n",
        "test_predictions_word2vec = clf_decision_word2vec.predict(test_features_word2vec)\n",
        "print(classification_report(Y_test['sentiment'],test_predictions_word2vec))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}